#aquatone
export PATH=$PATH:/root/tools/aquatone

#findomain
export PATH=$PATH:/root/tools/findomain

#gf

echo 'source $GOPATH/pkg/mod/github.com/tomnomnom/gf@v0.0.0-20200618134122-dcd4c361f9f5/gf-completion.zsh' >> ~/.zshrc



#Asset Discovery[1-5]




#curl -> Searching the target domain on cert.sh and save the output to json
#jq & sed ->Slicing and filtering the output, so we got only the domain
#jq is a command line JSON processor that is used to extract certain pieces of data deom the returned response
#sed is used to remove double quotes and hosts prepended with *
#sort -> eliminating duplicate domains and sorted
#tee -> Reads from the standard input and append to the give files

1(){
#Subdomain enumeration [6 tools]
#amass
amass enum --passive -d $1 > amass.txt

#findomain
findomain-linux -t $1 -u findomain.txt

#crtsh
curl -s https://crt.sh/\?q\=\%.$1\&output\=json | jq -r '.[].name_value' | sed 's/\*\.//g' | sort -u | tee -a ./crtsh.txt

#assetfinder
assetfinder --subs-only $1 > assetfinder.txt

#subfinder
subfinder -d $1 -o subfinder.txt

#sublist3r
python3 ~/tools/Sublist3r/sublist3r.py -d $1 -t 10 -v -o sublist3r.txt

#Sort subdomain
cat amass.txt findomain.txt crtsh.txt assetfinder.txt subfinder.txt sublist3r.txt | sort -u > subdomain.txt

#httprobe (Checking whether domain is alive or not)
cat subdomain.txt | httprobe > httprobe.txt

#Massdns (to get ip for all subdomains )
~/tools/massdns/./bin/massdns -r ~/tools/massdns/lists/resolvers.txt -t A -o S -w massdns.out subdomain.txt
#Output will look like  events.samjoy.in. A 201.10.208.14

#To get only IP
cat massdns.out | awk '{print $3}' | sort -u | grep -oE "\b([0-9]{1,3}\.){3}[0-9]{1,3}\b" > ips-online.txt

#cat $1 | awk '{print $1}' | sed 's/.$//' | sort -u >final.txt
#Used awk to get the first part



}


2-single(){

#find parameters
#paramspider
python3 ~/tools/ParamSpider/paramspider.py --d $1 -o paramspider.txt



#crawl links
#gau
gau $1 | qsreplace -a | tee gau.txt

#gau with filtering
cat gau.txt | grep "=" | egrep -iv ".(jpg|jpeg|gif|css|tif|tiff|png|ttf|woff|woff2|ico|pdf|svg|txt)" | tee gau-filter.txt

#Waybackurls
echo $1 |waybackurls | qsreplace -a |tee waybackurls.txt

#Waybackurls with filtering
cat waybackurls.txt | grep "=" | egrep -iv ".(jpg|jpeg|gif|css|tif|tiff|png|ttf|woff|woff2|ico|pdf|svg|txt)" | tee waybackurls-filter.txt

cat gau-filter.txt waybackurls-filter.txt | qsreplace -a |tee injection.txt



}


#xss-----------------------------------------------------------------------------------------

xss-1(){


#kxss [1-4]
cat injection.txt | /root/go/bin/./kxss | sed 's/=.*/=/' | sed 's/URL: //' | tee gw-kxss.txt
cat output/paramspider.txt | /root/go/bin/./kxss | sed 's/=.*/=/' | sed 's/URL: //' | tee ps-kxss.txt

#gau and wayback
dalfox file gw-kxss.txt | tee result-xss-1.txt

dalfox -b gabriel.xss.ht file gw-kxss.txt | tee result-xss-2.txt

#paramspider
dalfox file ps-kxss.txt | tee result-xss-3.txt

dalfox -b gabriel.xss.ht file ps-kxss.txt | tee result-xss-4.txt

}

xss-2(){

#gfpattern [5-8]
cat injection.txt | gf xss | sed 's/=.*/=/' | tee gw-gf.txt
cat output/paramspider.txt | gf xss | sed 's/=.*/=/' | tee ps-gf.txt

#gau and wayback
dalfox file gw-gf.txt | tee result-xss-5.txt

dalfox -b gabriel.xss.ht file gw-gf.txt | tee result-xss-6.txt

#paramspider
dalfox file gf-paramspider-xss.txt | tee result-xss-7.txt

dalfox -b gabriel.xss.ht file gf-paramspider-xss.txt | tee result-xss-8.txt



}



xss-3(){
#no filter (Agressive)

#gau and wayback
dalfox file injection.txt | tee result-xss-9.txt

dalfox -b gabriel.xss.ht file injection.txt | tee result-xss-10.txt


#paramspider
dalfox file output/paramspider.txt | tee result-xss-11.txt

dalfox -b gabriel.xss.ht file output/paramspider.txt | tee result-xss-12.txt

}


#xss-----------------------------------------------------------------------------------------






testing(){


}


remain(){
gospider -s "https://www.eucerin.nl/" -c 10 -d 1 --other-source | tee rls.txt

cat rls.txt | grep "=" | egrep -iv ".(jpg|jpeg|gif|css|tif|tiff|png|ttf|woff|woff2|ico|pdf|svg|txt)" | tee spider.txt

}







3-Multiple domain(){
#find parameters
#paramspider
cat subdomain.txt | xargs -I % python3 ~/tools/ParamSpider/paramspider.py -l high -o % -d % ;

#crawl links
#gau
cat $1 | gau | sort -u > gau.txt

#gau with filtering
cat gau.txt | grep "=" | egrep -iv ".(jpg|jpeg|gif|css|tif|tiff|png|ttf|woff|woff2|ico|pdf|svg|txt)" | tee gau-filter.txt

#Waybackurls
cat $1 |waybackurls | sort -u | tee waybackurls.txt

#Waybackurls with filtering
cat waybackurls.txt | grep "=" | egrep -iv ".(jpg|jpeg|gif|css|tif|tiff|png|ttf|woff|woff2|ico|pdf|svg|txt)" | tee testxss.txt

#Go spider

#Filter JS files
#getjs
cat $1 |getJS >javascriptfile.txt

}




gospider-1(){
gospider -S $1 -c 10 -d 5 --blacklist ".(jpg|jpeg|gif|css|tif|tiff|png|ttf|woff|woff2|ico|pdf|svg|txt)" --other-source | grep -e "code-200" | awk '{print $5}'|grep "=" | qsreplace -a | dalfox pipe -o result.txt
}

arjun-1(){
arjun -i $1 -oT arjun1.txt
}

arjun-2(){
arjun -u $1 -m GET -oT arjun-get.txt
arjun -u $1 -m POST -oT arjun-post.txt
arjun -u $1 -m JSON -oT arjun-json.txt
}


dalfox1(){
dalfox -b gabriel.xss.ht file $1

}



1.2-crtsh(){

}


1.3-subfinder(){

}



1.4-sublist3r() {

}


#This will be merged, sorted and duplicates will be removed and written to File
1.5-sort-subdomain() {
cat $1 $2 $3 $4 | sort -u > final-list.txt
}



#At this stage we have huge list of potential subdomains for your target.
#To verify that, you need to run an active DNS resolution, for that MassDNS is the perfect tool ie)to determine which of the hosts are actually online
#NOTE: 100% percent manual verification is often recommended because we can't rely on automated tools for this process

1.6-httprobe() {
cat $1 | httprobe > httprobe.txt
}

1.7-massdns(){

~/tools/massdns/./bin/massdns -r ~/tools/massdns/lists/resolvers.txt -t A -o S -w massdns.out $1
#Output will look like  events.samjoy.in. A 201.10.208.14

#To get only IP
cat massdns.out | awk '{print $3}' | sort -u | grep -oE "\b([0-9]{1,3}\.){3}[0-9]{1,3}\b" > ips-online.txt

#cat $1 | awk '{print $1}' | sed 's/.$//' | sort -u >final.txt
#Used awk to get the first part
}



#-------------------------End of Asset Discovery--------------------------


#-----------------------Port Scan -------------------------------------------

2.1-rustscan(){

rustscan 'scan-ip.txt' -p --ulimit 5000 -- -n -sV -Pn -oA scan-result

}




2.2-massscan(){

masscan -iL $1 -p1-65535 --rate=10000 -oL masscan.txt

}















#-----------------------Port Scan -------------------------------------------

aqua() {
cat $1 | aquatone -out /root/recon/aquatone
}

#URL and parameter discovery

#URL Discovery
getallurl(){
cat $1 | gau -o gau.txt
}



wayback(){
cat $1 |waybackurls >waybackurls.txt
}


#Parameter

parameterspider(){

python3 ~/tools/ParamSpider/paramspider.py --domain $1 --output paramspider.txt
}


arjunfinder(){
arjun -i $1 -oT arjun.txt
}

getjs(){
cat $1 |getJS >javascriptfile.txt
}
















#----- AWS -------

s3ls(){
aws s3 ls s3://$1
}

s3cp(){
aws s3 cp $2 s3://$1
}

#---- Content discovery ----
thewadl(){ #this grabs endpoints from a application.wadl and puts them in yahooapi.txt
curl -s $1 | grep path | sed -n "s/.*resource path=\"\(.*\)\".*/\1/p" | tee -a ~/tools/dirsearch/db/yahooapi.txt
}

#----- recon -----
crtndstry(){
./tools/crtndstry/crtndstry $1
}



mscan(){
sudo masscan -p4443,2075,2076,6443,3868,3366,8443,8080,9443,9091,3000,8000,5900,8081,6000,10000,8181,3306,5000,4000,8888,5432,15672,9999,161,4044,7077,4040,9000,8089,443,74

}

certspotter(){
curl -s https://certspotter.com/api/v0/certs\?domain\=$1 | jq '.[].dns_names[]' | sed 's/\"//g' | sed 's/\*\.//g' | sort -u | grep $1
} #h/t Michiel Prins



certnmap(){
curl https://certspotter.com/api/v0/certs\?domain\=$1 | jq '.[].dns_names[]' | sed 's/\"//g' | sed 's/\*\.//g' | sort -u | grep $1  | nmap -T5 -Pn -sS -i - -$
} #h/t Jobert Abma

ipinfo(){
curl http://ipinfo.io/$1
}


#------ Tools ------
dirsearch(){ #runs dirsearch and takes host and extension as arguments
python3 ~/tools/dirsearch/dirsearch.py -u $1 -e php,txt,zip,html,js -t 50 -b --format html -o dirsearch.html
}

sqlmap(){
python ~/tools/sqlmap*/sqlmap.py -u $1
}

ncx(){
nc -l -n -vv -p $1 -k
}

crtshdirsearch(){ #gets all domains from crtsh, runs httprobe and then dir bruteforcers
curl -s https://crt.sh/?q\=%.$1\&output\=json | jq -r '.[].name_value' | sed 's/\*\.//g' | sort -u | httprobe -c 50 | grep https | xargs -n1 -I{} python3 ~/tools/dirsearch/dirsearch.py -u {} -e $2 -t 50 -b }

